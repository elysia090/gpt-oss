Sera Transfer Kit (STK) — Precise GPU-free Migration Spec
Goal: deterministically project open-weight Transformer checkpoints (e.g., MoE or dense) into Sera’s constant-time runtime using only CPU. No training, no GPU, no unbounded loops. Outputs are binary arrays + a manifest that Sera can publish by a single pointer swap.
	1.	Scope and guarantees
1.1 All steps are O(N) over model size with fixed loop bounds per inner kernel; runtime Sera hot path remains O(1)/event.
1.2 Determinism: fixed PRNG (splitmix64), fixed reduction orders, pinned threads, fixed BLAS mode, fixed byte layouts, fixed tie-breakers.
1.3 Memory: stream tensors by layer via mmap; peak RAM bounded by max(layer_bytes) + small scratch (<=512 MiB).
1.4 Failure policy: any bound breach => disable optional features (e.g., O(0) route) and emit invariant record; conversion must still complete.
	2.	Inputs, outputs, and binary formats
2.1 Inputs (read-only)
input/model.safetensors | pytorch_model.bin (mmap)
input/config.json (d_model, n_heads, head_dim, vocab_size, rope/alibi flags, moe metadata if any)
input/tokenizer.model | {merges.txt, vocab.json} (optional for FST build)
2.2 Outputs (write-once)
output/sera_manifest.bin
output/arrays/*.bin as below
2.3 Common array header (fixed 64 bytes, little-endian)
u32 magic = 0x53455241  // “SERA”
u16 dtype  // 1=f64,2=f32,3=i32,4=i16,5=i8,6=u8,7=q8_8,8=q4_12,9=bf16
u16 rank
u64 dims[5]  // unused entries = 1
u64 byte_len
u64 crc32c
u64 sha256_low64
u32 flags  // bit0=row_major, bit1=aligned_64B, bit2=ftz_on
u32 reserved
Bytes immediately follow, 64-byte aligned. All arrays row-major if flag set.
2.4 File list (all with Common header)
tokenizer_fst.bin, T_1.bin..T_Ltok.bin
prf_W.bin [r x d_model, f32 or f64]
R_init.bin [r x d_v, f32], s_init.bin [r, f32]
whitening_mu.bin [r,f32], whitening_sig2.bin [r,f32]
overlays_H.bin [r x r_v, f32], overlays_U.bin [d_v x r_v, f32], overlays_DeltaW.bin [r_v x r_v, f32]
linear_mphf.bin, linear_keys.bin [C,u64], linear_weights.bin [C,f32], cuckoo_delta.bin
memory_coeff.bin [{mode:ARMA, p<=2,q<=1, coeffs f64}], delaybuf_init.bin
bridge_hubs.bin [contexts x H_bits, u64], bridge_qDin.bin, bridge_qDout.bin, peer_scores.bin [Q8.8]
2.5 Manifest (fixed fields, no “version” field; schema digest is stored)
magic=0x5345524D  // “SERM”
endian=LE, abi_flags(FMA/FTZ), seed_digest, schema_sha256
tokenizer{L_tok,S_norm,L_norm,P_gen,sp_cert_digest,unicode_policy}
prf{r,tau,whitening_eps,clip_c}
denominator{beta_floor,lambda_star_digest}
linear{C,d,b,S,L_cuckoo,Q,tau_low,tau_high}
memory{mode in {SOS,ARMA},p,q,L,T_phi,K,pole_radius_min}
overlays{P,k_max,r_v}
ccr_cert{gamma_CCR_target, m_target, norm_def}
bridge{K,W,Proj_shape,Proj_digest,beta_min,beta_max,guard{margin_policy,eps_row_policy},store_limits{load_max,stash_max,kick_max}}
search{A_max,A_cap,H_sel,H_roll,c_puct,epsilon,vloss}
capacity{lambda_hat,T_rb_ms,slack,margin}
salts{mphf_salt_digest,key_salt_digest}
hashes{prev_hash,curr_hash}
All scalar fields are bounded; any missing optional array implies feature disabled.
	3.	Tensor name mapping (deterministic)
3.1 Name patterns are configured via regex list in config.json; each pattern maps to a role tag in {EMB,Q,K,V,O,FFN_W1,FFN_W2,FFN_B1,FFN_B2,LN_G,LN_B,ROUTER_GATE,ROPE_ALPHA}.
3.2 Resolution rule: first matching regex wins; ties resolved by lexical order of pattern string; absence => role omitted.
3.3 MoE: if ROUTER_GATE exists, experts E are enumerated in sorted order by derived magnitude (see 7).
	4.	Tokenizer build (CPU-only)
4.1 If source is BPE byte-level: extract bytepieces of length 1..L_tok; else use EXT wrapper (no FST build, still emit proposal set).
4.2 Sardinas–Patterson (build-time)
Construct sets S_k until empty or epsilon appears; store sp_cert_digest (sha256 of minimal witness). If epsilon found => reject vocab.
4.3 Rolling hashes RH_n
64-bit Karp-Rabin mod 2^64 with base B=257; precompute P[i]=B^i mod 2^64 for i<=L_tok; update is branchless.
4.4 Minimal perfect hash per n (two-level CHD style)
Partition keys into buckets by h1(key)=mix64(key) mod M, M=ceil(1.23*K). For each bucket b:
find seed s_b (increment from 0) s.t. secondary positions h2_i=(mix64(key,s_b)+i) mod M are all free (i in {0,1}); deterministic search cap 2^16.
Serialize seeds array and base table; lookup is 2 probes. Collisions are guarded by bytewise Dec equality.
4.5 Encoder per byte: at most L_tok table probes; accept only if Dec[id] matches; no backtracking; residual flush at EOF.
4.6 Generator proposals C_t: build frequency table from merges/vocab if available; else fill by splitmix64 over id space; store P_gen ids in manifest.
	5.	PRF projection and whitening (no data required)
5.1 PRNG
splitmix64(x): x += 0x9E3779B97F4A7C15; z=(x^(x>>30))*0xBF58476D1CE4E5B9; z=(z^(z>>27))*0x94D049BB133111EB; return z^(z>>31)
Gaussian pair via Box-Muller using two 64-bit uniforms mapped to (0,1) with u= (x>>11)*2^-53, v=(y>>11)*2^-53; use cos/sin deterministic libcall; clamp u in [2^-52,1-2^-52].
5.2 Diagonal pre-whitener from W_K
For each head h with matrix W_K[h] in R^{head_dim x d_model_block}:
d_h[j] = mean_i (W_K[h]_{i,j}^2)
D = blockdiag(d_h) across heads; set d_j := max(d_j, eps_wk) with eps_wk=1e-8.
5.3 PRF basis
Draw r standard Gaussians w_i in R^{d_model}; scale by D^{1/2}: w_i := sqrt(d).w_i; store prf_W[r x d_model].
tau from config; define phi_i(x)=r^(-1/2)exp( w_i^T x / sqrt(tau) - ||x||^2/(2tau) ).
5.4 Whitening stats init
whitening_mu[:] = 0; whitening_sig2[j] = 1 for all j (or fold D to r by random projection once: v = prf_W * ones / ||.||; set sig2 = 1 + 0.1|v|).
5.5 Floors
Choose beta_floor = max(1e-3, 5e-4 * expected_den); record denominator schedule digest; den always >= beta_floor at runtime.
	6.	Value overlays from W_O (rank-r_v)
6.1 Fixed H and U
H in R^{r x r_v}: take first r_v columns of the deterministic Hadamard-like basis generated by splitmix64, orthonormalized by Householder QR with fixed sign convention (R diag positive).
U in R^{d_v x r_v}: thin orthonormal basis from W_O via deterministic randomized range finder:
draw Omega in R^{d_v x r_v} via splitmix64; Y=W_O^T * Omega; Q=householder_qr(Y); U=Q[:,0:r_v]
6.2 Core DeltaW
Solve least-squares in r_v-space for ΔW such that U*(H*ΔW)^T approximates W_O:
Let A = H, B = U, target T = W_O
Compute Z = argmin_Z ||T - B Z^T A^T||_F
Closed form with orthonormal A,B: Z = B^T T A
Store overlays_H=H, overlays_U=U, overlays_DeltaW=Z; runtime deltaC(q) = U * ( (phi_w(q)^T H) * Z ).
	7.	FFN collapse to injective sparse dictionary
7.1 Per layer gather W1 (h x d), b1 (h), W2 (d x h), b2 (d). Activation GELU is approximated by ReLU in collapse (fast upper bound).
7.2 Influence score
S = |W2| * |W1|  // dims (d x d); compute by blocked GEMM with abs on the fly; use i8 packing allowed
7.3 Top-L selection per input feature j
Take indices I_j of top L entries in column j of S by magnitude; compute signed effect
e_j = sum_{h in I_j} sign(W2[:,h]) .* max(0, W1[h,j])
Accumulate e_j into base weight vector W_base[:,j]; keep residual into delta set R_j (indices not selected)
7.4 Injective addressing
Stable feature ids are u64 keys: key = layer_id<<32 | j
Build MPHF h() over all live keys by two-level method in 4.4 with bounded seed search; serialize h seeds.
Allocate base arrays: weights[C], indices implicit via h(key). Serialize linear_keys for audit only.
7.5 Delta dictionary (bounded cuckoo)
Params (d choices, b slots, stash S, relocation cap L_cuckoo=8, ring Q cap = C/32)
Insert residual pairs (key, val) in deterministic order; kicks <= L_cuckoo; overflow to stash, then ring; emergency slot count at most 1 per shard.
Serialize cuckoo_delta with compact bucket arrays. Lookup probes <= d*b + S + 1.
7.6 Bias
Global b := sum of layer biases projected by collapse; stored in a small side record.
	8.	Positional encoding to finite rational memory
8.1 If RoPE with base θ_i per head (from config or inferred)
Use AR(2) with complex-conjugate poles at radius ρ and angle θ̄=mean_i θ_i:
Choose damping ρ=0.995; difference eq: y_t = 2ρcosθ̄ * y_{t-1} - ρ^2 * y_{t-2} + u_t
Coeffs stored as {a1=2ρcosθ̄, a2=−ρ^2, b0=1}. Poles < 1 guaranteed.
8.2 If ALiBi or learned abs
Fit SOS (L<=6) by vector-fit on synthetic impulse response of attention score to 1-token shift with fixed seed; accept first set with pole radii < 1; else scale all a_i by 0.95 until stable.
8.3 Delay buffer
T_phi and K from manifest; delaybuf_init zeros; accumulators use Kahan compensation in f64; single reduction site back to f32 if requested.
	9.	Bridge hubs, routes, and peer scoring (cold start)
9.1 Hubs
H_bits = 64W where W in {2,3}. If ROUTER_GATE present:
expert norm m_e = ||W_gate[:,e]||_2; pick top H experts; map to hub ids [0..H-1].
Else: hubs = splitmix64-derived pseudo-experts.
9.2 Token/context to hubs
For token id t and context id c: two independent hashes with salts -> two set bits per leg in W words; store in bridge_hubs bitset.
9.3 qDin/qDout quantization
Quantize leg scores rowwise to Q8.8: q = clip(round(xs), -32768, 32767) with s chosen so that p99 of |x| maps to 0.832767; store s_in, s_out per row. eps_row = 0.5(1/s_in + 1/s_out).
9.4 Guard preload for promoted pairs (optional)
For top-K pairs per token (by hashed score), precompute margin m and competitor bounds C_h and leg update bounds U,V from capacity targets; store compact tuples for O(0) path.
9.5 Peer IQ/HON
Initialize to 0.5 (Q8.8=128) with seed-jitter ±2; serialize peer_scores.bin.
	10.	Deterministic scheduling, threads, and numerics
10.1 Threads pinned to [0..T-1]; BLAS set to T; OMP/MKL threads fixed; affinity documented in manifest; no dynamic allocation inside inner loops.
10.2 Floating point contract: IEEE-754 double for reductions; f32 storage unless explicitly f64; FMA and FTZ flags set and recorded; libm calls routed through a pinned implementation.
10.3 Reduction orders fixed: lexicographic layer then tensor, row-major within tensor; QR and SVD routines use Householder with sign convention R diag > 0.
10.4 PRNG streams: each stage has unique counter tuple (module_id, layer_id, local_counter); no data-dependent reseeding.
	11.	Hot conversion algorithms (pseudocode, fixed loops)

function build_prf_W(d_model, r, D_diag):
x = seed0
for i in 0..r-1:
u = toUnit(splitmix64(x)); v = toUnit(splitmix64(x+=C))
g0,g1 = box_muller(u,v)
for j in 0..d_model-1:  // fixed stride
prf_W[i,j] = g0 * sqrt(max(D_diag[j],1e-8))
x += C
save(prf_W)

function overlays_from_WO(W_O, r, r_v, d_v):
H = hadamard_like(r,r_v,seedH); H = householder_qr_orth(H)
Omega = gaussian(d_v,r_v,seedU)
Y = gemm(transpose(W_O), Omega)
U = householder_qr_orth(Y)[:,0:r_v]
Z = transpose(U) * W_O * H   // exact with orthonormal U,H
save(H,U,Z)

function collapse_ffn(W1,W2):
// compute S = |W2| * |W1|
S = blocked_abs_gemm(|W2|, |W1|)
for j in 0..d-1:
I = topL_indices(S[:,j], L)  // fixed partial selection
e = 0
for h in I:
e += sign_col(W2,h) * relu_row(W1,h,j)
add_to_base_dict(j,e)
spill_residuals_to_cuckoo(j,S[:,j] \ I)

function mphf_build(keys):
M = ceil(1.23*|keys|)
buckets = group_by(h1(key)=mix64(key) mod M)
for b in 0..M-1:
s=0
while s<65536:
pos0 = mix64(seed=s,key=b) mod M
pos1 = mix64(seed=s+1,key=b) mod M
if both_free(pos0,pos1): assign; break
s+=1
record_seed(b,s)
serialize seeds; lookup uses two positions and Dec equality guard

function hubs_and_routes(token_ids, H_bits, W):
for each id:
for leg in 0..W-1:
bit = mix64(id,leg) >> (64-6)  // 0..63
set bit in word[leg]
	12.	Determinism checks and invariants
12.1 Tokenizer: SP certificate hash matches; probe bound <= L_tok; Greedy==Decode on random bytes (seeded).
12.2 PRF: prf_W SHA256 recorded; whitening_sig2 >= eps; beta_floor > 0.
12.3 Overlays: H,U orthonormal to 1e-6; Z finite; deltaC path returns finite for random q.
12.4 Linear: MPHF seeds found; probes <= d*b+S+1; no emergency overflow except allowed 1 per shard; base+delta reconstructs e_j within 0.5% MAPE on synthetic inputs.
12.5 Memory: pole radii < 1; DF-II-T states bounded in double; single reduction site back to f32.
12.6 Bridge: hub_and_p99 <= K; quant scales sane; guard tuples consistent (m>0 implies C_h < m).
12.7 Manifest: schema digest equals computed; all array sizes consistent; crc32c and sha256_low64 pass.
	13.	Complexity and resource envelopes (20B-class dense or MoE small)
13.1 Tokenizer FST build: O(|V|L_tok), ~1–2 s, <1 GiB peak.
13.2 PRF build r=512: O(rd_model), a few 100 ms; arrays ~rd_model4 bytes.
13.3 Overlays r_v<=16: two GEMMs of size d_v x r_v and d_v x d_model projected; ~seconds CPU.
13.4 FFN collapse: blocked GEMM per layer; total ~minutes CPU; memory streamed.
13.5 MPHF+cuckoo: O(C) with small constants; seconds.
13.6 Memory fit: sub-second.
13.7 Bridge init: seconds.
	14.	Fast path choices (quality vs speed, all deterministic)
14.1 r in {256,512,1024}; r=512 is a good default.
14.2 r_v in {8,12,16}; 12 often sufficient.
14.3 FFN top-L in {8,12}; higher L yields better base coverage, fewer deltas.
14.4 ρ in {0.99,0.995} for AR(2) damping if RoPE missing.
14.5 K in {4,6}; W in {2,3}.
	15.	CLI and stage graph
15.1 CLI
stk convert –in input –out output –r 512 –rv 12 –Ltok 8 –topL 12 –threads 16
stk check   –out output
stk replay  –out output –prompt “Hello.”
15.2 Stages (fixed DAG)
parse_tensors -> tokenizer_build_or_wrap -> prf_build -> overlays_build -> ffn_collapse -> mphf_cuckoo -> memory_fit -> bridge_init -> manifest_emit -> check
Each stage emits its own crc32c and sha256_low64; manifest stores both.
	16.	Acceptance thresholds (cold start, no data)
16.1 PRF vs softmax surrogate on synthetic q,K,V: relative L2 error <= 1e-2 averaged over 1k trials.
16.2 Linear collapse MAPE <= 0.5% on random sparse inputs with density <= 0.01.
16.3 Memory AR(2) impulse alignment: cosine similarity >= 0.97 vs RoPE surrogate at 32-step window.
16.4 Bridge guard precheck pass-rate >= 0.9 on synthetic margins.
16.5 Deterministic replay: two full runs produce identical sha256_low64 for all arrays and identical Diagnostics snapshots.
	17.	Hook-up to Sera ABI (no glue code divergence)
17.1 Arrays and manifest names exactly match Sera section keys.
17.2 Publication: Sera loads arrays, validates checksums, writes single generation pointer; readers pin and proceed.
17.3 If any optional array missing, the corresponding module auto-disables and emits beta=0 on that path.
	18.	Troubleshooting rules (constant-time fixes)
18.1 MPHF seed search exhausted: increase M by 5% and rerun only MPH stage; all other arrays unchanged.
18.2 Overlays orthonormality fail: rerun QR with stabilized Householder (fixed tolerance 1e-12).
18.3 Poles >= 1: multiply all AR coeffs by 0.95 repeatedly (max 3 times), then accept.
18.4 Guard margins negative: mark those pairs non-promoted; O(1) two-hop still available.
18.5 CRC mismatch: recompute header; never rewrite payloads after first write.
	19.	Minimal reproducible recipe (20B-class, laptop CPU)
export OMP_NUM_THREADS=16; export MKL_NUM_THREADS=16
stk convert –in input –out output –r 512 –rv 12 –Ltok 8 –topL 12 –threads 16
stk check –out output
stk replay –out output –prompt “A short test.”
Expected artifacts: ~1–3 GiB total arrays depending on d_model and r; total wall time minutes, bounded by FFN collapse GEMMs; zero GPU usage.

